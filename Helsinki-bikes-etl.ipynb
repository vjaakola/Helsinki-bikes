{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext, GroupedData\n",
    "from pyspark.sql.functions import *\n",
    "#from pyspark.sql.functions import col, unix_timestamp, round\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "from etl_clean_create_dim_tables import Operators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "#os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Build spark session\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to s3 bucket\n",
    "#input_data = \"s3://helsinki-bikes/\"\n",
    "#output_data = \"s3://helsinki-bikes/results/\" \n",
    "\n",
    "# path to on-premises disk storage\n",
    "input_data = 'helsinki_bikes/'\n",
    "output_data = 'helsinki_bikes/results/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and read the dataset\n",
    "bikes_data = input_data + '/bikes_data/2017_2019/*.csv'\n",
    "bikes_df = spark.read.csv(bikes_data, inferSchema=True, header=True, sep=',', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9217647"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bikes_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean bikes data and create dimension tables and prepare views  for 10 and 60 min intervals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clean and create bike trips from 2017 to 2019 and write parguet files\n",
    "bikes_from_2017_to_2019 = Operators.df_clean_create_orig(bikes_df, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and create dim bikes 10 min interval\n",
    "bikes_df_clean_10min = Operators.df_clean_create_10min(bikes_df, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and create dim bikes 60 min interval\n",
    "bikes_df_clean_60min = Operators.df_clean_create_60min(bikes_df, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bike trips for every id\n",
    "bike_count = bikes_from_2017_to_2019.groupby('id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum bike trips to 10 min interval\n",
    "bike_count_10min = bikes_df_clean_10min.groupby('date').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum bike trips to 60 min interval\n",
    "bike_count_60min = bikes_df_clean_60min.groupby('date').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean weather data and create dimesion tables and prepare views for 10 and 60 min intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weather dataset\n",
    "temp_data = input_data + '/weather/2017_2019/*.csv'\n",
    "temp_df = spark.read.csv(temp_data,header=True, sep=',' ,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88116"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_df_clean_10min = Operators.temp_clean_create_10min(temp_df, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_clean_60min = Operators.temp_clean_create_60min(temp_df_clean_10min, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count average weather conditions for 60 min\n",
    "temp_df_clean_60min.createOrReplaceTempView('temp_hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_60min =spark.sql(\"\"\"\n",
    "    SELECT temp_hour.date, \n",
    "    avg(temp_hour.air_temp) as air_temp,\n",
    "    avg(temp_hour.cloud_amount) as cloud_amount,\n",
    "    avg(temp_hour.pressure) as pressure,\n",
    "    avg(temp_hour.humidity) as humidity,\n",
    "    avg(temp_hour.precipitation) as precipitation,\n",
    "    avg(temp_hour.deW_point_temp) as dev_point_temp,\n",
    "    avg(temp_hour.visibility) as visibility,\n",
    "    avg(temp_hour.wind_direc) as wind_direc,\n",
    "    avg(temp_hour.gust_speed) as gust_speed,\n",
    "    avg(temp_hour.wind_speed) as wind_speed\n",
    "    \n",
    "    \n",
    "    FROM temp_hour\n",
    "    GROUP by temp_hour.date\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and create stations dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stations dataset\n",
    "stations_df = input_data + '/stations/2019/*.csv'\n",
    "stations_df = spark.read.csv(stations_df,header=True, sep=',',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_df_clean = Operators.stations_clean_create(stations_df, output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create fact table and machine learning tables and write parguet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create views\n",
    "bikes_from_2017_to_2019.createOrReplaceTempView('bikes_view')\n",
    "stations_df_clean.createOrReplaceTempView('stations_view') \n",
    "bike_count.createOrReplaceTempView('count_view') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the fact table by joining bikes view, stations view and count view\n",
    "\n",
    "bikes_fact_table = spark.sql(\"\"\"\n",
    "SELECT \n",
    "        bikes_view.id,\n",
    "        bikes_view.year,\n",
    "        bikes_view.month,\n",
    "        bikes_view.day,\n",
    "        bikes_view.weekday,\n",
    "        bikes_view.hour,\n",
    "        bikes_view.dep_station_name,\n",
    "        bikes_view.departure,\n",
    "        bikes_view.return,\n",
    "        bikes_view.ret_station_name,\n",
    "        bikes_view.dep_station_id,\n",
    "        bikes_view.ret_station_id,\n",
    "        bikes_view.distance,\n",
    "        bikes_view.duration,\n",
    "        stations_view.city,\n",
    "        stations_view.capacity,\n",
    "        stations_view.longitude,\n",
    "        stations_view.latitude,\n",
    "        count_view.count\n",
    "    \n",
    "   \n",
    "\n",
    "FROM bikes_view\n",
    "left JOIN stations_view ON bikes_view.dep_station_id = stations_view.station_id\n",
    "JOIN  count_view ON bikes_view.id = count_view.id\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# write bikes_fact_table parquet files\n",
    "bikes_fact_table.write.mode('overwrite').partitionBy('year').parquet(output_data + 'bikes_fact_2017_2019')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning table with 10 min interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bikes view from cleaned 60 min dataframe\n",
    "bikes_df_clean_10min.createOrReplaceTempView('bikes_10min')\n",
    "\n",
    "# create temp view from cleaned 60 min dataframe\n",
    "temp_df_clean_10min.createOrReplaceTempView('temp_10min')\n",
    "\n",
    "# create count view from bike count 10 min dataframe\n",
    "bike_count_10min.createOrReplaceTempView('count_10min')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create machine learning table for 10 min interval\n",
    "ml_bikes_10min = spark.sql(\"\"\"\n",
    "SELECT  distinct\n",
    "        bikes_10min.date,\n",
    "        bikes_10min.year,\n",
    "        bikes_10min.month,\n",
    "        bikes_10min.day,\n",
    "        bikes_10min.weekday,\n",
    "        bikes_10min.hour,\n",
    "        temp_10min.air_temp,\n",
    "        temp_10min.humidity,\n",
    "        temp_10min.wind_speed,\n",
    "        count_10min.count as bike_count\n",
    "        \n",
    "   \n",
    "\n",
    "FROM bikes_10min\n",
    "inner JOIN temp_10min ON bikes_10min.date = temp_10min.date\n",
    "left JOIN count_10min ON bikes_10min.date = count_10min.date \n",
    "AND count_10min.date = temp_10min.date\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write ml_bikes_10min parquet files\n",
    "ml_bikes_10min.write.mode('overwrite').partitionBy('year').parquet(output_data + 'ml_bikes_10min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning table with 60 min interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bikes view from cleaned 60 min dataframe\n",
    "bikes_df_clean_60min.createOrReplaceTempView('bikes_60min')\n",
    "\n",
    "# create temp view from cleaned 60 min dataframe\n",
    "temp_60min.createOrReplaceTempView('temp_60min')\n",
    "\n",
    "# create count view from bike count 60 min dataframe\n",
    "bike_count_60min.createOrReplaceTempView('count_60min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create machine learning table for 60 min interval\n",
    "ml_bikes_60min = spark.sql(\"\"\"\n",
    "SELECT  distinct\n",
    "        bikes_60min.date,\n",
    "        bikes_60min.year,\n",
    "        bikes_60min.month,\n",
    "        bikes_60min.day,\n",
    "        bikes_60min.weekday,\n",
    "        bikes_60min.hour,\n",
    "        temp_60min.air_temp,\n",
    "        temp_60min.humidity,\n",
    "        temp_60min.wind_speed,\n",
    "        count_60min.count as bike_count\n",
    "        \n",
    "   \n",
    "\n",
    "FROM bikes_60min\n",
    "inner JOIN temp_60min ON bikes_60min.date = temp_60min.date\n",
    "left JOIN count_60min ON bikes_60min.date = count_60min.date \n",
    "AND count_60min.date = temp_60min.date\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write ml_bikes_60min parquet files\n",
    "ml_bikes_60min.write.mode('overwrite').partitionBy('year').parquet(output_data + 'ml_bikes_60min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "Data quality checks consits of count checks to ensure completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing all the checks, data looks good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality check passed for bikes original dataset with 9217647 records\n",
      "Data quality check passed for bikes 10min dimension table with 9217647 records\n",
      "Data quality check passed for bikes 60min dimension table with 9217647 records\n",
      "Data quality check passed for temperature 10min dimension table with 88116 records\n",
      "Data quality check passed for temperature 60 min dataframe with 88116 records\n",
      "Data quality check passed for stations dimension table with 351 records\n",
      "Data quality check passed for bikes fact table with 9217647 records\n",
      "Data quality check passed for ml 10min table with 85945 records\n",
      "Data quality check passed for ml 60min table with 14454 records\n"
     ]
    }
   ],
   "source": [
    "def quality_check(df, description):\n",
    "    '''\n",
    "    Input: Spark dataframes, dimension's and a fact table with descriptions\n",
    "    \n",
    "    Output: Outcome of data quality check\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    result = df.count()\n",
    "    if result == 0:\n",
    "        print(\"Data quality check failed for {} with zero records\".format(description))\n",
    "    else:\n",
    "        print(\"Data quality check passed for {} with {} records\".format(description, result))\n",
    "    return \n",
    "\n",
    "# Process data quality check\n",
    "quality_check(bikes_df, \"bikes original dataset\")\n",
    "quality_check(bikes_df_clean_10min, \"bikes 10min dimension table\")\n",
    "quality_check(bikes_df_clean_60min, \"bikes 60min dimension table\")\n",
    "quality_check(temp_df_clean_10min, \"temperature 10min dimension table\")\n",
    "quality_check(temp_df_clean_60min, \"temperature 60 min dataframe\")\n",
    "quality_check(stations_df_clean, \"stations dimension table\")\n",
    "quality_check(bikes_fact_table, \"bikes fact table\")\n",
    "quality_check(ml_bikes_10min, \"ml 10min table\")\n",
    "quality_check(ml_bikes_60min, \"ml 60min table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every 10 min slots during time period are togther 88128\n",
    "# every 60 min slots during time period are togther 14688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
