{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# ETL Pipeline for Helsinki Bikes\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The goal is to create a database where data scientists or data analysts can describe or make predictions. The created database can answer e.g. for the following questions:\n",
    "\n",
    "1. What are the rush hours for bike stations?\n",
    "2. How to predict, how many bikes are needed in which stations and when?\n",
    "3. How many bike trips were made?\n",
    "4. What are the distances and durations on bike trips?\n",
    "5. How weather conditions affect to bike trips?\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext, GroupedData\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.sql.functions import col, unix_timestamp, round\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "In this project, we are gathering data from city bikes of Helsinki and weather. The main idea is to clean and prepare datasets for data scientists and data analysts that they can make innovative descriptions and predictions. First, I made a simple ETL pipeline, that is very straight forward. Later on, it is possible to use and evaluate the same pipeline to prepare all historical data and make OLAP versions for predicting ongoing bike business.\n",
    "\n",
    "I made quite a simple and effective ETL pipeline, but it took a lot of time to try and test with a spark. I used Spark for processing big files and Pandas for making insights into the data. Data was first in the AWS S3 bucket and then it was cleaned and processed to create dimension and fact tables. After processing, these files were transformed back as parquet files to S3. AWS EMR cluster was needed during this process.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "Dataset can be found here:\n",
    "City bike stations’ Origin-Destination (OD) data includes all trips made with city bikes of Helsinki and Espoo. The data includes information about the trip’s origin and destination stations, start and end times, distance (in meters) as well as duration (in seconds). https://hri.fi/data/en_GB/dataset/helsingin-ja-espoon-kaupunkipyorilla-ajatut-matkat\n",
    "Finnish Meteorological Institute Instantaneous weather observations are available from 2010, daily, and monthly observations from the 1960s onwards (depending on weather station). https://en.ilmatieteenlaitos.fi/download-observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Build spark session\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_data = \"s3://helsinki-bikes/\"\n",
    "output_data = \"s3://helsinki-bikes/results/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load and read the dataset\n",
    "bikes_data = input_data + '/bikes_data/2019/*.csv'\n",
    "df = spark.read.csv(bikes_data, inferSchema=True, header=True, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3787948"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Departure</th>\n",
       "      <th>Return</th>\n",
       "      <th>Departure station id</th>\n",
       "      <th>Departure station name</th>\n",
       "      <th>Return station id</th>\n",
       "      <th>Return station name</th>\n",
       "      <th>Covered distance (m)</th>\n",
       "      <th>Duration (sec.)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-06-30 23:59:49</td>\n",
       "      <td>2019-07-01 00:11:39</td>\n",
       "      <td>51</td>\n",
       "      <td>Itälahdenkatu</td>\n",
       "      <td>60</td>\n",
       "      <td>Porkkalankatu</td>\n",
       "      <td>2229.0</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-06-30 23:59:21</td>\n",
       "      <td>2019-07-01 00:01:17</td>\n",
       "      <td>118</td>\n",
       "      <td>Fleminginkatu</td>\n",
       "      <td>117</td>\n",
       "      <td>Brahen puistikko</td>\n",
       "      <td>289.0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-06-30 23:59:19</td>\n",
       "      <td>2019-07-01 00:36:33</td>\n",
       "      <td>54</td>\n",
       "      <td>Gyldenintie</td>\n",
       "      <td>17</td>\n",
       "      <td>Varsapuistikko</td>\n",
       "      <td>4872.0</td>\n",
       "      <td>2230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Departure              Return  Departure station id  \\\n",
       "0 2019-06-30 23:59:49 2019-07-01 00:11:39                    51   \n",
       "1 2019-06-30 23:59:21 2019-07-01 00:01:17                   118   \n",
       "2 2019-06-30 23:59:19 2019-07-01 00:36:33                    54   \n",
       "\n",
       "  Departure station name  Return station id Return station name  \\\n",
       "0          Itälahdenkatu                 60       Porkkalankatu   \n",
       "1          Fleminginkatu                117    Brahen puistikko   \n",
       "2            Gyldenintie                 17      Varsapuistikko   \n",
       "\n",
       "   Covered distance (m)  Duration (sec.)  \n",
       "0                2229.0              706  \n",
       "1                 289.0              112  \n",
       "2                4872.0             2230  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load weather dataset\n",
    "temp_df = input_data + '/weather/2019/*.csv'\n",
    "temp_df = spark.read.csv(temp_df,header=True, sep=',' ,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30810"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time zone</th>\n",
       "      <th>Cloud amount (1/8)</th>\n",
       "      <th>Pressure (msl) (hPa)</th>\n",
       "      <th>Relative humidity (%)</th>\n",
       "      <th>Precipitation intensity (mm/h)</th>\n",
       "      <th>Air temperature (degC)</th>\n",
       "      <th>Dew-point temperature (degC)</th>\n",
       "      <th>Horizontal visibility (m)</th>\n",
       "      <th>Wind direction (deg)</th>\n",
       "      <th>Gust speed (m/s)</th>\n",
       "      <th>Wind speed (m/s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-01 00:10:00</td>\n",
       "      <td>UTC</td>\n",
       "      <td>3.0</td>\n",
       "      <td>996.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>13.5</td>\n",
       "      <td>47280.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-01 00:20:00</td>\n",
       "      <td>UTC</td>\n",
       "      <td>3.0</td>\n",
       "      <td>995.8</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.2</td>\n",
       "      <td>13.5</td>\n",
       "      <td>49920.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-07-01 00:30:00</td>\n",
       "      <td>UTC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>995.6</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.3</td>\n",
       "      <td>13.5</td>\n",
       "      <td>39970.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-01 00:40:00</td>\n",
       "      <td>UTC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>995.7</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.3</td>\n",
       "      <td>13.4</td>\n",
       "      <td>43530.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-01 00:50:00</td>\n",
       "      <td>UTC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>995.4</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.2</td>\n",
       "      <td>13.1</td>\n",
       "      <td>43860.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date Time zone Cloud amount (1/8) Pressure (msl) (hPa)  \\\n",
       "0  2019-07-01 00:10:00       UTC                3.0                996.0   \n",
       "1  2019-07-01 00:20:00       UTC                3.0                995.8   \n",
       "2  2019-07-01 00:30:00       UTC                1.0                995.6   \n",
       "3  2019-07-01 00:40:00       UTC                1.0                995.7   \n",
       "4  2019-07-01 00:50:00       UTC                1.0                995.4   \n",
       "\n",
       "  Relative humidity (%) Precipitation intensity (mm/h) Air temperature (degC)  \\\n",
       "0                  83.0                            0.0                   16.5   \n",
       "1                  84.0                            0.0                   16.2   \n",
       "2                  84.0                            0.0                   16.3   \n",
       "3                  83.0                            0.0                   16.3   \n",
       "4                  82.0                            0.0                   16.2   \n",
       "\n",
       "  Dew-point temperature (degC) Horizontal visibility (m) Wind direction (deg)  \\\n",
       "0                         13.5                   47280.0                196.0   \n",
       "1                         13.5                   49920.0                196.0   \n",
       "2                         13.5                   39970.0                192.0   \n",
       "3                         13.4                   43530.0                194.0   \n",
       "4                         13.1                   43860.0                206.0   \n",
       "\n",
       "  Gust speed (m/s) Wind speed (m/s)  \n",
       "0              6.1              3.8  \n",
       "1              6.4              3.3  \n",
       "2              5.4              3.2  \n",
       "3              5.8              3.7  \n",
       "4              7.1              4.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_pd = temp_df.toPandas()\n",
    "temp_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load stations dataset\n",
    "stations_df = input_data + '/stations/2019/*.csv'\n",
    "stations_df = spark.read.csv(stations_df,header=True, sep=',',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FID</th>\n",
       "      <th>ID</th>\n",
       "      <th>Nimi</th>\n",
       "      <th>Namn</th>\n",
       "      <th>Name</th>\n",
       "      <th>Osoite</th>\n",
       "      <th>Adress</th>\n",
       "      <th>Kaupunki</th>\n",
       "      <th>Stad</th>\n",
       "      <th>Operaattor</th>\n",
       "      <th>Kapasiteet</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>501</td>\n",
       "      <td>Hanasaari</td>\n",
       "      <td>Hanaholmen</td>\n",
       "      <td>Hanasaari</td>\n",
       "      <td>Hanasaarenranta 1</td>\n",
       "      <td>Hanaholmsstranden 1</td>\n",
       "      <td>Espoo</td>\n",
       "      <td>Esbo</td>\n",
       "      <td>CityBike Finland</td>\n",
       "      <td>10</td>\n",
       "      <td>24.840319</td>\n",
       "      <td>60.16582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>503</td>\n",
       "      <td>Keilalahti</td>\n",
       "      <td>Kägelviken</td>\n",
       "      <td>Keilalahti</td>\n",
       "      <td>Keilalahdentie 2</td>\n",
       "      <td>Kägelviksvägen 2</td>\n",
       "      <td>Espoo</td>\n",
       "      <td>Esbo</td>\n",
       "      <td>CityBike Finland</td>\n",
       "      <td>28</td>\n",
       "      <td>24.827467</td>\n",
       "      <td>60.171524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>505</td>\n",
       "      <td>Westendinasema</td>\n",
       "      <td>Westendstationen</td>\n",
       "      <td>Westendinasema</td>\n",
       "      <td>Westendintie 1</td>\n",
       "      <td>Westendvägen 1</td>\n",
       "      <td>Espoo</td>\n",
       "      <td>Esbo</td>\n",
       "      <td>CityBike Finland</td>\n",
       "      <td>16</td>\n",
       "      <td>24.805758</td>\n",
       "      <td>60.168266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FID   ID            Nimi               Namn            Name  \\\n",
       "0   1  501       Hanasaari         Hanaholmen       Hanasaari   \n",
       "1   2  503      Keilalahti         Kägelviken      Keilalahti   \n",
       "2   3  505  Westendinasema   Westendstationen  Westendinasema   \n",
       "\n",
       "              Osoite               Adress Kaupunki  Stad        Operaattor  \\\n",
       "0  Hanasaarenranta 1  Hanaholmsstranden 1    Espoo  Esbo  CityBike Finland   \n",
       "1   Keilalahdentie 2     Kägelviksvägen 2    Espoo  Esbo  CityBike Finland   \n",
       "2     Westendintie 1       Westendvägen 1    Espoo  Esbo  CityBike Finland   \n",
       "\n",
       "  Kapasiteet          x          y  \n",
       "0         10  24.840319   60.16582  \n",
       "1         28  24.827467  60.171524  \n",
       "2         16  24.805758  60.168266  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_pd = stations_df.toPandas()\n",
    "stations_pd.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "\n",
    "I made a preliminary (added DateTime to weather data files) data exploration and founded out, that these datasets had quite minimal issues with missing or duplicated values. From the quality checks (end of this notebook, we can see how many records were removed. Anyhow making the clean dataset for creating tables was not so easy task. Spark had some odd features and it was very challenging to find the right workflow. It took many tries and errors.\n",
    "\n",
    "The main thing with cleaning the data was to cast the right data types and setting up DateTime (timestamp data type) relation with bikes data and weather data (temp_df). Rounding timestamps with pySpark is not so easy as python. It looks simple now, but it was hard to find answers form Stackoverflow or anywhere, had to do many tries.  \n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Performing cleaning tasks here\n",
    "Bikes data:\n",
    "- cast right data types\n",
    "- rename columns\n",
    "- create new features: extract year, month, day, weekday and hour from the timestamp \n",
    "- create rounded date column from original timestamp to make relation with weather data\n",
    "- fill NaN values with 0\n",
    "\n",
    "Weather data:\n",
    "- cast right data types\n",
    "- rename columns\n",
    "- fill NaN values with 0\n",
    "\n",
    "Stations data:\n",
    "- cast right data types\n",
    "- rename columns\n",
    "- fill NaN values with 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Cleaning Bikes dataset and creating tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Departure: timestamp (nullable = true)\n",
      " |-- Return: timestamp (nullable = true)\n",
      " |-- Departure station id: integer (nullable = true)\n",
      " |-- Departure station name: string (nullable = true)\n",
      " |-- Return station id: integer (nullable = true)\n",
      " |-- Return station name: string (nullable = true)\n",
      " |-- Covered distance (m): double (nullable = true)\n",
      " |-- Duration (sec.): integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time zone: string (nullable = true)\n",
      " |-- Cloud amount (1/8): string (nullable = true)\n",
      " |-- Pressure (msl) (hPa): string (nullable = true)\n",
      " |-- Relative humidity (%): string (nullable = true)\n",
      " |-- Precipitation intensity (mm/h): string (nullable = true)\n",
      " |-- Air temperature (degC): string (nullable = true)\n",
      " |-- Dew-point temperature (degC): string (nullable = true)\n",
      " |-- Horizontal visibility (m): string (nullable = true)\n",
      " |-- Wind direction (deg): string (nullable = true)\n",
      " |-- Gust speed (m/s): string (nullable = true)\n",
      " |-- Wind speed (m/s): string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FID: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Nimi: string (nullable = true)\n",
      " |-- Namn: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Osoite: string (nullable = true)\n",
      " |-- Adress: string (nullable = true)\n",
      " |-- Kaupunki: string (nullable = true)\n",
      " |-- Stad: string (nullable = true)\n",
      " |-- Operaattor: string (nullable = true)\n",
      " |-- Kapasiteet: string (nullable = true)\n",
      " |-- x: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "For the main purpose the simplified star schema model suits best. I prepared a fact table to start quick analysis and with those exploration it is easy to follow new paths.\n",
    "\n",
    "Fact table (fact_table) 3 787 377 records, 1.4.-31.102019\n",
    "- date\n",
    "- year\n",
    "- month\n",
    "- day\n",
    "- weekday\n",
    "- hour\n",
    "- departure station name\n",
    "- departure time\n",
    "- return time\n",
    "- return station name\n",
    "- departure station id\n",
    "- return station id\n",
    "- distance\n",
    "- duration\n",
    "- air temperature\n",
    "- humidity\n",
    "- wind speed\n",
    "- longitude\n",
    "- latitude\n",
    "\n",
    "Bikes dimension table (dim_bikes) 3 787 946 records\n",
    "- departure station name\n",
    "- departure time\n",
    "- return time\n",
    "- return station name\n",
    "- distance\n",
    "- duration\n",
    "- date\n",
    "- year\n",
    "- month\n",
    "- day\n",
    "- weekday\n",
    "- hour\n",
    "\n",
    "Weather dimension table (dim_temp) 30 810 records, 1.4.-31.102019\n",
    "- date\n",
    "- cloud amount\n",
    "- pressure (msl)\n",
    "- relative humidity\n",
    "- precipitation intensity\n",
    "- air temperature\n",
    "- dew-point temperature\n",
    "- horizontal visibility\n",
    "- wind direction\n",
    "- gust speed\n",
    "- wind speed\n",
    "\n",
    "Stations dimension table (dim_stations) 351 records\n",
    "- station id\n",
    "- city\n",
    "- name\n",
    "- address\n",
    "- operator\n",
    "- capacity\n",
    "- longitude\n",
    "- latitude\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Use efective way to clean and create tables\n",
    "1. Bikes data in dataframe df to clean and prepare for creating table \n",
    "2. Weather data in dataframe temp_df to clean and prepare for creating table \n",
    "3. Stations data in dataframe temp_df to clean and prepare for creating table \n",
    "4. Create tempView to create dimension bikes table \n",
    "5. Create tempView to create dimension weather table \n",
    "6. Create tempView to create dimension stations table \n",
    "7. Create tempView to create a fact table\n",
    "8. Process data quality checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# change the data types & create new features for futher analysis\n",
    "# rename and cast data types to columns to align with data model \n",
    "df = df\\\n",
    "        .withColumn('departure', col('Departure').cast('timestamp'))\\\n",
    "        .withColumn('return', col('Return').cast('timestamp'))\\\n",
    "        .withColumnRenamed('Departure station id', 'dep_station_id') \\\n",
    "        .withColumnRenamed('Departure station name', 'dep_station_name') \\\n",
    "        .withColumn('ret_station_id', col('Return station id').cast('integer'))\\\n",
    "        .withColumnRenamed('Return station name', 'ret_station_name') \\\n",
    "        .withColumn('distance', col('Covered distance (m)').cast('integer'))\\\n",
    "        .withColumnRenamed('Duration (sec.)', 'duration')\\\n",
    "        .withColumn('date', ((round(unix_timestamp(col(\"departure\")) / 600) * 600).cast('timestamp'))) \\\n",
    "        .withColumn('year', year('date'))\\\n",
    "        .withColumn('month', month('date'))\\\n",
    "        .withColumn('day', dayofweek('date'))\\\n",
    "        .withColumn('weekday', date_format(col('date'), 'EEEE'))\\\n",
    "        .withColumn('hour', hour('date'))\\\n",
    "        .drop('Departure station id', 'Departure station name','Return station id','Return station name','Covered distance (m)','Duration (sec.)')\\\n",
    "        .fillna(0)\\\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# rename and cast data types to columns to align with data model \n",
    "temp_df = temp_df\\\n",
    "        .withColumn('date', col('Date').cast('timestamp'))\\\n",
    "        .withColumn('cloud_amount', col('Cloud amount (1/8)').cast('float'))\\\n",
    "        .withColumn('pressure', col('Pressure (msl) (hPa)').cast('float'))\\\n",
    "        .withColumn('humidity', col('Relative humidity (%)').cast('float'))\\\n",
    "        .withColumn('precipitation', col('Precipitation intensity (mm/h)').cast('float'))\\\n",
    "        .withColumn('air_temp', col('Air temperature (degC)').cast('float'))\\\n",
    "        .withColumn('dev_point_temp', col('Dew-point temperature (degC)').cast('float'))\\\n",
    "        .withColumn('visibility', col('Horizontal visibility (m)').cast('float'))\\\n",
    "        .withColumn('wind_direc', col('Wind direction (deg)').cast('float'))\\\n",
    "        .withColumn('gust_speed', col('Gust speed (m/s)').cast('float'))\\\n",
    "        .withColumn('wind_speed', col('Wind speed (m/s)').cast('float'))\\\n",
    "        .withColumnRenamed('Time zone', 'time_zone')\\\n",
    "        .drop('Cloud amount (1/8)','Pressure (msl) (hPa)','Relative humidity (%)','Precipitation intensity (mm/h)','Air temperature (degC)', 'Dew-point temperature (degC)','Horizontal visibility (m)','Wind direction (deg)','Gust speed (m/s)','Wind speed (m/s)','Time zone' )\\\n",
    "        .fillna(0)\\\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# rename and cast data types to columns to align with data model \n",
    "stations_df = stations_df\\\n",
    "        .withColumn('station_id', col('ID').cast('integer'))\\\n",
    "        .withColumn('name', col('Nimi').cast('string'))\\\n",
    "        .withColumnRenamed('Osoite', 'address')\\\n",
    "        .withColumnRenamed('Kaupunki', 'city')\\\n",
    "        .withColumnRenamed('Operaattor', 'operator')\\\n",
    "        .withColumn('capacity', col('Kapasiteet').cast('integer'))\\\n",
    "        .withColumn('longitude', col('x').cast('float'))\\\n",
    "        .withColumn('latitude', col('y').cast('float'))\\\n",
    "        .fillna(0)\\\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create views & tables to write parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create bikes view from cleaned dataframe df\n",
    "df.createOrReplaceTempView('bikes_view')\n",
    "\n",
    "# create bikes dimension table\n",
    "dim_bikes = spark.sql('''\n",
    "\n",
    "SELECT distinct \n",
    "        bikes_view.dep_station_name,\n",
    "        bikes_view.departure,\n",
    "        bikes_view.return,\n",
    "        bikes_view.ret_station_name,\n",
    "        bikes_view.distance,\n",
    "        bikes_view.duration,\n",
    "        bikes_view.date,\n",
    "        bikes_view.year,\n",
    "        bikes_view.month,\n",
    "        bikes_view.day,\n",
    "        bikes_view.weekday,\n",
    "        bikes_view.hour\n",
    "      \n",
    "\n",
    "FROM bikes_view\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write dim_bikes to parquet files\n",
    "dim_bikes.write.mode('overwrite').parquet(output_data + 'dim_bikes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create temperature view from cleaned dataframe temp_df\n",
    "temp_df.createOrReplaceTempView('temp_view')\n",
    "\n",
    "# create temperatures dimension table\n",
    "dim_temp = spark.sql('''\n",
    "\n",
    "SELECT \n",
    "        temp_view.date,\n",
    "        temp_view.cloud_amount,\n",
    "        temp_view.pressure,\n",
    "        temp_view.humidity,\n",
    "        temp_view.precipitation,\n",
    "        temp_view.air_temp,\n",
    "        temp_view.dev_point_temp,\n",
    "        temp_view.visibility,\n",
    "        temp_view.wind_direc,\n",
    "        temp_view.gust_speed,\n",
    "        temp_view.wind_speed\n",
    "        \n",
    "FROM temp_view\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write dim_temp parquet files\n",
    "temp_df.write.mode('overwrite').parquet(output_data + 'dim_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create stations view from cleaned dataframe stations_df\n",
    "stations_df.createOrReplaceTempView('stations_view')\n",
    "\n",
    "dim_stations = spark.sql('''\n",
    "\n",
    "SELECT distinct \n",
    "        stations_view.station_id,\n",
    "        stations_view.city,\n",
    "        stations_view.name,\n",
    "        stations_view.address,\n",
    "        stations_view.operator,\n",
    "        stations_view.capacity,\n",
    "        stations_view.longitude,\n",
    "        stations_view.latitude\n",
    "        \n",
    "        \n",
    "        \n",
    "FROM stations_view\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write dim_stations parquet files\n",
    "stations_df.write.mode('overwrite').parquet(output_data + 'dim_stations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create bikes view from cleaned dataframe stations_df\n",
    "#df.createOrReplaceTempView('bikes_view')\n",
    "#temp_df.createOrReplaceTempView('temp_view')\n",
    "\n",
    "# Create the fact table by joining bikes view, temp viev and stations view\n",
    "\n",
    "fact_table = spark.sql('''\n",
    "SELECT  distinct\n",
    "        bikes_view.date,\n",
    "        bikes_view.year,\n",
    "        bikes_view.month,\n",
    "        bikes_view.day,\n",
    "        bikes_view.weekday,\n",
    "        bikes_view.hour,\n",
    "        bikes_view.dep_station_name,\n",
    "        bikes_view.departure,\n",
    "        bikes_view.return,\n",
    "        bikes_view.ret_station_name,\n",
    "        bikes_view.dep_station_id,\n",
    "        bikes_view.ret_station_id,\n",
    "        bikes_view.distance,\n",
    "        bikes_view.duration,\n",
    "        temp_view.air_temp,\n",
    "        temp_view.humidity,\n",
    "        temp_view.wind_speed,\n",
    "        stations_view.longitude,\n",
    "        stations_view.latitude\n",
    "        \n",
    "   \n",
    "\n",
    "FROM bikes_view\n",
    "inner JOIN temp_view ON bikes_view.date = temp_view.date\n",
    "JOIN stations_view ON bikes_view.dep_station_id = stations_view.station_id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "Data quality checks consits of count checks to ensure completeness\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "After finishing all the checks, data looks good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality check passed for bikes dataset with 3787948 records\n",
      "Data quality check passed for bikes dimension table with 3787946 records\n",
      "Data quality check passed for temperature dataset with 30810 records\n",
      "Data quality check passed for temperature dimension table with 30810 records\n",
      "Data quality check passed for stations dataset with 351 records\n",
      "Data quality check passed for bikes fact table with 3787377 records\n"
     ]
    }
   ],
   "source": [
    "def quality_check(df, description):\n",
    "    '''\n",
    "    Input: Spark dataframes, dimension's and a fact table with descriptions\n",
    "    \n",
    "    Output: Outcome of data quality check\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    result = df.count()\n",
    "    if result == 0:\n",
    "        print(\"Data quality check failed for {} with zero records\".format(description))\n",
    "    else:\n",
    "        print(\"Data quality check passed for {} with {} records\".format(description, result))\n",
    "    return \n",
    "\n",
    "# Process data quality check\n",
    "quality_check(df, \"bikes dataset\")\n",
    "quality_check(dim_bikes, \"bikes dimension table\")\n",
    "quality_check(temp_df, \"temperature dataset\")\n",
    "quality_check(dim_temp, \"temperature dimension table\")\n",
    "quality_check(stations_df, \"stations dataset\")\n",
    "quality_check(fact_table, \"bikes fact table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Data dictionary is in the file Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "Spark was a very good technolygy for handling big datasets. Spark SQL was also very simple way to create tables without having standard SQL operations.\n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "\n",
    "In this first phase data can be updated when analyst will need it. Later on it is possible update as often as needed, but the I'll have to make an automation such as Airflow can offer.\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "If the data was increased by 100x, I would no longer process the data like this. I would make an automated process for that, such as Airflow. Also a good implemention of data warehouse is needed then.\n",
    "\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "If the need of SLA is that data needs to populate a dashboard daily, then I surely would use a scheduling tool such as Airflow to run the ETL pipeline. \n",
    "\n",
    " * The database needed to be accessed by 100+ people.\n",
    " \n",
    "If the data needs to be accessed by 100+ people, I could use Spark, Hive, Spark sql template views, and ofcourse Apache Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
